{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?  \n",
        "- K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based machine learning algorithm.\n",
        "It works by finding the K closest data points to a new sample using a distance metric.\n",
        "\n",
        "- Classification:\n",
        "The class is decided by majority voting among K neighbors.\n",
        "\n",
        "- Regression:\n",
        "The output is the average (mean) of the K neighbors’ values.\n",
        "\n",
        "\n",
        "Q2: What is the Curse of Dimensionality and how does it affect KNN performance?   \n",
        "-  The Curse of Dimensionality means that as the number of features increases:\n",
        "\n",
        "    Distance between points becomes less meaningful\n",
        "\n",
        "    All points appear almost equally distant\n",
        "\n",
        "    KNN accuracy degrades and computation increases                                                                             \n",
        "\n",
        "Q3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "- Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms features into new orthogonal components that capture maximum variance.\n",
        "    | Feature Selection         | PCA                      |\n",
        "    | ------------------------- | ------------------------ |\n",
        "    | Selects existing features | Creates new features     |\n",
        "    | Uses subset               | Uses linear combinations |\n",
        "    | Interpretable             | Less interpretable       |\n",
        "\n",
        "\n",
        "\n",
        "Q4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "- Eigenvectors: Directions of maximum variance (principal components)\n",
        "\n",
        "- Eigenvalues: Amount of variance captured by each eigenvector\n",
        "\n",
        "    Higher eigenvalue ⇒ more important component\n",
        "    "
      ],
      "metadata": {
        "id": "1zEq3lGdaR-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "KNN and PCA work well together because PCA prepares the data, and KNN performs better decisions on that prepared data.\n",
        "\n",
        "- PCA (Principal Component Analysis) reduces the number of features in the Wine dataset by transforming the original chemical measurements into a smaller set of uncorrelated principal components that retain most of the variance.\n",
        "\n",
        "- This solves the curse of dimensionality, removes noise, and eliminates redundant features.\n",
        "\n",
        "- KNN, being a distance-based algorithm, benefits directly from this because distances become more meaningful in a lower-dimensional space.\n",
        "\n",
        "- As a result, KNN becomes faster, less prone to overfitting, and often more accurate."
      ],
      "metadata": {
        "id": "7U1zdlHIdWfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "pred1 = knn.predict(X_test)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, pred1))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_sc = scaler.fit_transform(X_train)\n",
        "X_test_sc = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_sc, y_train)\n",
        "pred2 = knn.predict(X_test_sc)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, pred2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJa4JmZxZxiv",
        "outputId": "6ac8b8de-18eb-44a9-f260-34b714f09202"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train_sc)\n",
        "\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvkjYVYjZuj1",
        "outputId": "15916c01-30e3-4c5e-925d-ec1a5a6c5b98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio:\n",
            "[0.35900066 0.18691934 0.11606557 0.07371716 0.0665386  0.04854582\n",
            " 0.04195042 0.02683922 0.0234746  0.01889734 0.01715943 0.01262928\n",
            " 0.00826257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "pca2 = PCA(n_components=2)\n",
        "X_train_pca = pca2.fit_transform(X_train_sc)\n",
        "X_test_pca = pca2.transform(X_test_sc)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "print(\"Accuracy with PCA + KNN:\", accuracy_score(y_test, pred_pca))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOOWa3oLZnwg",
        "outputId": "27611b69-7ea1-44e4-d579-8e4603f51c47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA + KNN: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_sc, y_train)\n",
        "    preds = knn.predict(X_test_sc)\n",
        "    print(f\"Accuracy ({metric}):\", accuracy_score(y_test, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ2aIskTZjpD",
        "outputId": "b6f0cc15-7cda-4108-ddd9-9811643cf6f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (euclidean): 0.9444444444444444\n",
            "Accuracy (manhattan): 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "\n",
        "- Step-by-Step Pipeline\n",
        "    1. PCA for Dimensionality Reduction\n",
        "\n",
        "        Thousands of genes → reduce noise\n",
        "\n",
        "        Prevent overfitting\n",
        "\n",
        "    2. Choosing Number of Components\n",
        "\n",
        "        Use explained variance (90–95%)\n",
        "\n",
        "        Scree plot / cumulative variance\n",
        "\n",
        "    3. Apply KNN\n",
        "\n",
        "        Works well in reduced feature space\n",
        "\n",
        "        Simple and interpretable\n",
        "\n",
        "    4. Evaluation Metrics\n",
        "\n",
        "        Accuracy\n",
        "\n",
        "        ROC-AUC\n",
        "\n",
        "        F1-score (imbalanced data)\n",
        "\n",
        "    5. Business / Stakeholder Justification\n",
        "\n",
        "    “This pipeline reduces noise, prevents overfitting, improves generalization, and ensures reliable predictions in sensitive biomedical applications.”\n"
      ],
      "metadata": {
        "id": "96qGqG_PeSn_"
      }
    }
  ]
}