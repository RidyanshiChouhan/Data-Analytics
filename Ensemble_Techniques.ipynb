{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Ensemble Learning is a machine learning technique where multiple models (called weak learners) are combined to build a stronger and more accurate model.\n",
        "Instead of relying on a single model, ensemble methods aggregate predictions from many models to reduce errors, improve stability, and increase generalization.\n",
        "\n",
        "- Key idea:\n",
        "\n",
        "  “Many weak models together perform better than one strong model.”\n",
        "\n",
        "- Why it works:\n",
        "\n",
        "    Reduces variance (Bagging)\n",
        "\n",
        "    Reduces bias (Boosting)\n",
        "\n",
        "    Handles complex patterns better\n",
        "\n",
        "    More robust to noise and overfitting\n",
        "\n",
        "- Common ensemble methods:\n",
        "\n",
        "    Bagging (Random Forest)\n",
        "\n",
        "    Boosting (AdaBoost, Gradient Boosting, XGBoost)\n",
        "\n",
        "    Voting & Stacking\n",
        "Q2: What is the difference between Bagging and Boosting?     \n",
        "- | Aspect      | Bagging                      | Boosting                         |\n",
        "| ----------- | ---------------------------- | -------------------------------- |\n",
        "| Goal        | Reduce variance              | Reduce bias                      |\n",
        "| Training    | Models trained independently | Models trained sequentially      |\n",
        "| Data        | Random bootstrap samples     | Focuses on misclassified samples |\n",
        "| Overfitting | Good for overfitting models  | Can overfit if noisy             |\n",
        "| Example     | Random Forest                | AdaBoost, Gradient Boosting      |\n",
        "\n",
        "- “Bagging improves stability, while Boosting improves accuracy by learning from mistakes.”\n",
        "\n",
        "\n",
        "Q3: What is bootstrap sampling and what role does it play in Bagging methods  like Random Forest?\n",
        "- Bootstrap sampling is a technique where random samples are drawn with replacement from the original dataset.\n",
        "\n",
        "- Role in Bagging / Random Forest:\n",
        "\n",
        "    Each tree is trained on a different bootstrap sample\n",
        "\n",
        "    Some rows appear multiple times, some not at all\n",
        "\n",
        "    This creates diversity among models, reducing correlation\n",
        "\n",
        "    Leads to lower variance and better generalization\n",
        "\n",
        "Q4: What are Out-of-Bag (OOB) samples and how is OOB score used to  evaluate ensemble models?\n",
        "- Out-of-Bag (OOB) samples are data points not selected in a bootstrap sample.\n",
        "\n",
        "- Key points:\n",
        "\n",
        "    About 36% of data is OOB for each tree\n",
        "\n",
        "    Used as validation data\n",
        "\n",
        "    OOB score estimates model performance without separate test set\n",
        "\n",
        "- Why useful:\n",
        "\n",
        "    Saves data\n",
        "\n",
        "    Faster evaluation\n",
        "\n",
        "    Less overfitting risk\n",
        "Q5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.     \n",
        "- | Aspect             | Decision Tree     | Random Forest            |\n",
        "| ------------------ | ----------------- | ------------------------ |\n",
        "| Stability          | Unstable          | Stable                   |\n",
        "| Bias               | High              | Lower                    |\n",
        "| Overfitting        | High              | Low                      |\n",
        "| Feature importance | Based on one tree | Averaged over many trees |\n",
        "| Reliability        | Low               | High                     |\n",
        "\n",
        "Random Forest provides more reliable and robust feature importance.              \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GY8n_XTQCY69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using\n",
        "#    sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importance = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "top_5 = importance.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\\n\", top_5)\n",
        "\n"
      ],
      "metadata": {
        "id": "n8I9XgoODn2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96745428-e2cf-4d3c-9504-62f2369d0073"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            " worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7: Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging\n",
        "bag = BaggingClassifier( n_estimators=100, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Accuracy:\", bag_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "FX0NaETGESEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5292218f-dc18-4db3-f0f2-36263b98ae36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8: Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid = GridSearchCV(rf, params, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "cYpdb0nMEXtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72454cff-7a87-4185-eae7-f8963fe98763"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 50}\n",
            "Best Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9: Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California  Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "bag = BaggingRegressor(n_estimators=100)\n",
        "rf = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Bagging MSE:\", mean_squared_error(y_test, bag.predict(X_test)))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "xzgMONE7EgBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c7735e-4795-4608-fca3-4bdd45ef2313"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.24783658522634328\n",
            "Random Forest MSE: 0.24856295146428647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world  context.  \n",
        "\n",
        "- Step-by-Step Approach\n",
        "1. Choose Bagging or Boosting\n",
        "\n",
        "   - Start with Random Forest (Bagging) for stability\n",
        "\n",
        "    - Use Boosting if bias is high\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "    - Cross-validation\n",
        "\n",
        "    - Limit tree depth\n",
        "\n",
        "    - Increase number of estimators\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "    - Decision Trees (interpretable)\n",
        "\n",
        "    - Logistic Regression (baseline)\n",
        "\n",
        "4. Evaluate Performance\n",
        "\n",
        "    - Cross-validation\n",
        "\n",
        "    - ROC-AUC\n",
        "\n",
        "    - Precision-Recall\n",
        "\n",
        "    - Confusion Matrix\n",
        "\n",
        "5. Business Justification\n",
        "\n",
        "    - Better default detection\n",
        "\n",
        "    - Reduced financial risk\n",
        "\n",
        "    - Fairer credit decisions\n",
        "\n",
        "    - Regulatory-friendly explainability\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "69t6J1NcEEbb"
      }
    }
  ]
}